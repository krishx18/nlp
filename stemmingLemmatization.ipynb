{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/lib/python3.11/site-packages/bzl-2.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /opt/homebrew/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "# Install nltk\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"The scoring method being used above takes the count of each word and represents the word in the vector by the number of counts of that particular word. What does a word having high word count signify? Does this mean that the word is important in retrieving information about documents? The answer is NO. Let me explain, if a word occurs many times in a document but also along with many other documents in our dataset, maybe it is because this word is just a frequent word; not because it is relevant or meaningful. One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach is called term frequency-inverse document frequency or shortly known as Tf-Idf approach of scoring.TF-IDF is intended to reflect how relevant a term is in a given document. So how is Tf-Idf of a document in a dataset calculated?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The scoring method being used above takes the count of each word and represents the word in the vector by the number of counts of that particular word.',\n",
       " 'What does a word having high word count signify?',\n",
       " 'Does this mean that the word is important in retrieving information about documents?',\n",
       " 'The answer is NO.',\n",
       " 'Let me explain, if a word occurs many times in a document but also along with many other documents in our dataset, maybe it is because this word is just a frequent word; not because it is relevant or meaningful.',\n",
       " 'One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized.',\n",
       " 'This approach is called term frequency-inverse document frequency or shortly known as Tf-Idf approach of scoring.TF-IDF is intended to reflect how relevant a term is in a given document.',\n",
       " 'So how is Tf-Idf of a document in a dataset calculated?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's convert the above para to sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see so many \"?\", \".\" and other symbols in this above paragharph so we can remove them to filter the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's clean the whole corpus [removing special characters etc]\n",
    "import re\n",
    "corpus = []\n",
    "for cnt in range(len(sentences)):\n",
    "    temp = re.sub('[^a-zA-Z]',' ',sentences[cnt])\n",
    "    temp = temp.lower()\n",
    "    corpus.append(temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The re.sub() function in Python is commonly used for substituting patterns in strings based on RegEx.\n",
    "\n",
    "    Group capturing in RegEx allows for the selective replacement of specific parts of a matched pattern while keeping other part of the string intact.\n",
    "\n",
    "    The re.sub() function helps remove unnecessary text, convert text cases, clean input, correct spelling errors, and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the scoring method being used above takes the count of each word and represents the word in the vector by the number of counts of that particular word ',\n",
       " 'what does a word having high word count signify ',\n",
       " 'does this mean that the word is important in retrieving information about documents ',\n",
       " 'the answer is no ',\n",
       " 'let me explain  if a word occurs many times in a document but also along with many other documents in our dataset  maybe it is because this word is just a frequent word  not because it is relevant or meaningful ',\n",
       " 'one approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like  the  that are also frequent across all documents are penalized ',\n",
       " 'this approach is called term frequency inverse document frequency or shortly known as tf idf approach of scoring tf idf is intended to reflect how relevant a term is in a given document ',\n",
       " 'so how is tf idf of a document in a dataset calculated ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/krishangopal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemming_corpus = []\n",
    "for doc in corpus:\n",
    "    # word tokenization\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    temp = []\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            temp.append(stemmer.stem(word))\n",
    "    stemming_corpus.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['score',\n",
       "  'method',\n",
       "  'use',\n",
       "  'take',\n",
       "  'count',\n",
       "  'word',\n",
       "  'repres',\n",
       "  'word',\n",
       "  'vector',\n",
       "  'number',\n",
       "  'count',\n",
       "  'particular',\n",
       "  'word'],\n",
       " ['word', 'high', 'word', 'count', 'signifi'],\n",
       " ['mean', 'word', 'import', 'retriev', 'inform', 'document'],\n",
       " ['answer'],\n",
       " ['let',\n",
       "  'explain',\n",
       "  'word',\n",
       "  'occur',\n",
       "  'mani',\n",
       "  'time',\n",
       "  'document',\n",
       "  'also',\n",
       "  'along',\n",
       "  'mani',\n",
       "  'document',\n",
       "  'dataset',\n",
       "  'mayb',\n",
       "  'word',\n",
       "  'frequent',\n",
       "  'word',\n",
       "  'relev',\n",
       "  'meaning'],\n",
       " ['one',\n",
       "  'approach',\n",
       "  'rescal',\n",
       "  'frequenc',\n",
       "  'word',\n",
       "  'often',\n",
       "  'appear',\n",
       "  'document',\n",
       "  'score',\n",
       "  'frequent',\n",
       "  'word',\n",
       "  'like',\n",
       "  'also',\n",
       "  'frequent',\n",
       "  'across',\n",
       "  'document',\n",
       "  'penal'],\n",
       " ['approach',\n",
       "  'call',\n",
       "  'term',\n",
       "  'frequenc',\n",
       "  'invers',\n",
       "  'document',\n",
       "  'frequenc',\n",
       "  'shortli',\n",
       "  'known',\n",
       "  'tf',\n",
       "  'idf',\n",
       "  'approach',\n",
       "  'score',\n",
       "  'tf',\n",
       "  'idf',\n",
       "  'intend',\n",
       "  'reflect',\n",
       "  'relev',\n",
       "  'term',\n",
       "  'given',\n",
       "  'document'],\n",
       " ['tf', 'idf', 'document', 'dataset', 'calcul']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krishangopal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the grammar for lemmatization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring\n",
      "method\n",
      "used\n",
      "take\n",
      "count\n",
      "word\n",
      "represents\n",
      "word\n",
      "vector\n",
      "number\n",
      "count\n",
      "particular\n",
      "word\n",
      "word\n",
      "high\n",
      "word\n",
      "count\n",
      "signify\n",
      "mean\n",
      "word\n",
      "important\n",
      "retrieving\n",
      "information\n",
      "document\n",
      "answer\n",
      "let\n",
      "explain\n",
      "word\n",
      "occurs\n",
      "many\n",
      "time\n",
      "document\n",
      "also\n",
      "along\n",
      "many\n",
      "document\n",
      "dataset\n",
      "maybe\n",
      "word\n",
      "frequent\n",
      "word\n",
      "relevant\n",
      "meaningful\n",
      "one\n",
      "approach\n",
      "rescale\n",
      "frequency\n",
      "word\n",
      "often\n",
      "appear\n",
      "document\n",
      "score\n",
      "frequent\n",
      "word\n",
      "like\n",
      "also\n",
      "frequent\n",
      "across\n",
      "document\n",
      "penalized\n",
      "approach\n",
      "called\n",
      "term\n",
      "frequency\n",
      "inverse\n",
      "document\n",
      "frequency\n",
      "shortly\n",
      "known\n",
      "tf\n",
      "idf\n",
      "approach\n",
      "scoring\n",
      "tf\n",
      "idf\n",
      "intended\n",
      "reflect\n",
      "relevant\n",
      "term\n",
      "given\n",
      "document\n",
      "tf\n",
      "idf\n",
      "document\n",
      "dataset\n",
      "calculated\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    # word tokenization\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(lemma.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see word \"signify\" became \"signifi\" with stemming and \"signify\" with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
